{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb95cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "\n",
    "# Préparation des données\n",
    "### Charger les données\n",
    "def charger_donnees(nom_fichier: str, affichage: bool=False) -> str:\n",
    "    \"\"\"\n",
    "    Charge les données du corpus à partir d'un fichier et les renvoie sous forme de liste de chaînes de caractères.\n",
    "\n",
    "    Paramètres :\n",
    "        nom_fichier : Nom du fichier.\n",
    "        affichage : Si vrai, des informations sur les données seront affichées.\n",
    "    \n",
    "    Retour :\n",
    "        str : Les données chargées sous forme de chaîne de caractères.\n",
    "    \"\"\"\n",
    "    donnees = []\n",
    "\n",
    "    with open(nom_fichier, \"r\", encoding='utf-8') as f:\n",
    "        donnees = f.read()\n",
    "\n",
    "    if affichage:\n",
    "        print(\"Type de données :\", type(donnees))\n",
    "        print(f\"Nombre de lettres : {len(donnees):,d}\")\n",
    "        print(\"Premières 100 lettres des données\")\n",
    "        print(\"-\"*30)\n",
    "        display(donnees[0:100])\n",
    "        print(\"-\"*30)\n",
    "\n",
    "        print(\"Dernières 100 lettres des données\")\n",
    "        print(\"-\"*30)\n",
    "        display(donnees[-100:])\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "    return donnees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738a038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diviser_en_phrases(donnees: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Divise les données par saut de ligne \"\\n\"\n",
    "    \n",
    "    Paramètres :\n",
    "        donnees (str) : Les données d'entrée sous forme de chaîne de caractères.\n",
    "    \n",
    "    Retourne :\n",
    "        list[str] : Une liste de phrases.\n",
    "    \"\"\"\n",
    "    phrases = donnees.split('\\n')\n",
    "    phrases = [phrase.strip() for phrase in phrases if phrase.strip()]\n",
    "    \n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caaed129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer_phrase(phrase: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize une phrase en tokens.\n",
    "\n",
    "    Paramètres :\n",
    "        phrase (str) : La phrase d'entrée.\n",
    "        \n",
    "    Retourne :\n",
    "        List[str] : Une liste de tokens.\n",
    "    \"\"\"\n",
    "    phrase = phrase.lower()\n",
    "    phrase = re.sub(r\"[^\\w\\s]\", \"\", phrase)\n",
    "    tokens = phrase.split()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e91b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_donnees_tokenisees(donnees: str) -> tuple[list[str], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Tokenise les données en phrases et en mots.\n",
    "\n",
    "    Paramètres :\n",
    "        donnees (str) : Les données d'entrée sous forme de chaîne de caractères.\n",
    "\n",
    "    Retourne :\n",
    "        tuple[list[str], list[list[str]]] : Un tuple contenant la liste des phrases et la liste des phrases tokenisées.\n",
    "    \"\"\"\n",
    "    phrases = diviser_en_phrases(donnees)\n",
    "    \n",
    "    phrases_tokenisees = [tokenizer_phrase(p) for p in phrases]\n",
    "    \n",
    "    return phrases, phrases_tokenisees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25fc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_vocabulaire(donnees_tokenisees: list[list[str]]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Extrait le vocabulaire à partir d'une liste de données tokenisées.\n",
    "\n",
    "    Paramètres :\n",
    "        donnees_tokenisees (list[list[str]]) : Une liste de phrases tokenisées.\n",
    "\n",
    "    Retourne :\n",
    "        set[str] : Un ensemble contenant les mots uniques présents dans les données tokenisées.\n",
    "    \"\"\"\n",
    "    vocabulaire = set()\n",
    "    for sous_liste in donnees_tokenisees:\n",
    "        for element in sous_liste:\n",
    "            vocabulaire.add(element)\n",
    "    return vocabulaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a749a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diviser_donnees(donnees_tokenisees: list[list[str]], ratio_entrainement: float=0.8,\n",
    "                affichage: bool=False) -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Divise les données en ensembles d'entraînement et de test en fonction du ratio spécifié.\n",
    "    \n",
    "    Paramètres :\n",
    "        donnees_tokenisees (list[list[str]]) : Les données d'entrée sous forme d'une liste de listes de tokens.\n",
    "        ratio_entrainement (float) : Le ratio des données à utiliser pour l'entraînement (entre 0 et 1).\n",
    "        affichage (bool) : Si vrai, des informations sur la division seront affichées.\n",
    "        \n",
    "    Retourne :\n",
    "        Tuple[List[list[str]], List[list[str]]] : Les ensembles d'entraînement et de test sous forme de listes de phrases ou de séquences.\n",
    "    \"\"\"\n",
    "    taille_entrainement = int(len(donnees_tokenisees) * ratio_entrainement)\n",
    "    donnees_entrainement = donnees_tokenisees[:taille_entrainement]\n",
    "    donnees_test = donnees_tokenisees[taille_entrainement:]\n",
    "\n",
    "    if affichage:\n",
    "        print(f\"{len(donnees_tokenisees)} données sont divisées en {len(donnees_entrainement)} ensemble d'entraînement et {len(donnees_test)} ensemble de test\")\n",
    "\n",
    "        print(\"Premier échantillon d'entraînement :\")\n",
    "        print(donnees_entrainement[0])\n",
    "            \n",
    "        print(\"Premier échantillon de test :\")\n",
    "        print(donnees_test[0])\n",
    "    \n",
    "    return donnees_entrainement, donnees_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "809e8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_mot(mot: str, vocabulaire: set[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Vérifie si un mot est présent dans le vocabulaire.\n",
    "\n",
    "    Paramètres :\n",
    "        mot (str) : Le mot à vérifier.\n",
    "        vocabulaire (set[str]) : L'ensemble de mots représentant le vocabulaire.\n",
    "\n",
    "    Retourne :\n",
    "        bool : True si le mot est dans le vocabulaire, False sinon.\n",
    "    \"\"\"\n",
    "    return mot.lower() in vocabulaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d18f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculer_distance_edition(source: str, cible: str, cout_insertion: int=1, \n",
    "                            cout_suppression: int=1, cout_remplacement: int=2) -> tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Calcule la distance d'édition minimale entre deux mots.\n",
    "    \n",
    "    Paramètres :\n",
    "        source (str) : Le mot source.\n",
    "        cible (str) : Le mot cible.\n",
    "        cout_insertion (int) : Le coût de l'insertion (par défaut 1).\n",
    "        cout_suppression (int) : Le coût de la suppression (par défaut 1).\n",
    "        cout_remplacement (i nt) : Le coût du remplacement (par défaut 2).\n",
    "\n",
    "    Retourne :\n",
    "        D : une matrice de dimensions len(source)+1 par len(cible)+1 contenant les distances d'édition minimales\n",
    "        med : la distance d'édition minimale (med) nécessaire pour convertir la chaîne source en la cible\n",
    "    \"\"\"\n",
    "    m, n = len(source), len(cible)\n",
    "    D = np.zeros((m+1, n+1), dtype=int)\n",
    "    \n",
    "    for ligne in range(1, m+1):\n",
    "        D[ligne, 0] = D[ligne-1, 0] + cout_suppression\n",
    "        \n",
    "    for colonne in range(1, n+1):\n",
    "        D[0, colonne] = D[0, colonne-1] + cout_insertion\n",
    "        \n",
    "    for ligne in range(1, m+1):\n",
    "        for colonne in range(1, n+1):\n",
    "            cout_rempl = cout_remplacement\n",
    "            if source[ligne-1] == cible[colonne-1]:\n",
    "                cout_rempl = 0\n",
    "            D[ligne, colonne] = min(D[ligne-1, colonne] + cout_suppression, D[ligne, colonne-1] + cout_insertion, D[ligne-1, colonne-1] + cout_rempl)\n",
    "          \n",
    "    med = D[m, n]\n",
    "    \n",
    "    return D, med\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5cbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrections_une_modification(mot: str, vocabulaire: list[str]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Génère un ensemble de corrections possibles pour un mot mal orthographié avec une seule modification.\n",
    "\n",
    "    Paramètres :\n",
    "        mot (str) : Le mot mal orthographié.\n",
    "        vocabulaire (list[str]) : Une liste de mots du vocabulaire.\n",
    "\n",
    "    Retourne :\n",
    "        set[str] : Un ensemble de corrections possibles.\n",
    "    \"\"\"\n",
    "\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    decoupes = [(mot[:i], mot[i:]) for i in range(len(mot) + 1)]\n",
    "\n",
    "    suppressions = [gauche + droite[1:] for gauche, droite in decoupes if droite]\n",
    "    suppressions = [s for s in suppressions if est_mot(s, vocabulaire)]\n",
    "\n",
    "    insertions = [gauche + c + droite for gauche, droite in decoupes for c in alphabet]\n",
    "    insertions = [i for i in insertions if est_mot(i, vocabulaire)]\n",
    "\n",
    "    remplacements = [gauche + c + droite[1:] for gauche, droite in decoupes if droite for c in alphabet]\n",
    "    remplacements = [r for r in remplacements if est_mot(r, vocabulaire)]\n",
    "\n",
    "    transpositions = [gauche + droite[1] + droite[0] + droite[2:] for gauche, droite in decoupes if len(droite) > 1]\n",
    "    transpositions = [t for t in transpositions if est_mot(t, vocabulaire)]\n",
    "    \n",
    "    return set(suppressions + insertions + remplacements + transpositions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0f9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_corrections(mot: str, vocabulaire: list[str], n_modifications: int=1, \n",
    "                        distance_max: int=2) -> set[str]:\n",
    "    \"\"\"\n",
    "    Génère une liste de corrections possibles pour un mot mal orthographié en fonction du vocabulaire donné et de la distance d'édition maximale.\n",
    "\n",
    "    Paramètres :\n",
    "        mot (str) : Le mot mal orthographié.\n",
    "        vocabulaire (list[str]) : Une liste de mots du vocabulaire.\n",
    "        n_modifications (int) : Le nombre de modifications autorisées pour générer des corrections (par défaut 1).\n",
    "        distance_max (int) : La distance d'édition maximale autorisée pour qu'une correction soit considérée (par défaut 2).\n",
    "\n",
    "    Retourne :\n",
    "        corrections_possibles (set[str]) : Un ensemble de corrections possibles.\n",
    "    \"\"\"\n",
    "    corrections_possibles = set()\n",
    "\n",
    "    if n_modifications == 1:\n",
    "        corrections_possibles = {corr for corr in corrections_une_modification(mot, vocabulaire) if calculer_distance_edition(mot, corr)[1] <= distance_max}\n",
    "    else:\n",
    "        modifications_precedentes = {mot}\n",
    "        for _ in range(n_modifications):\n",
    "            modifications_actuelles = set()\n",
    "            for modification_precedente in modifications_precedentes:\n",
    "                nouvelles_corrections = {corr for corr in corrections_une_modification(modification_precedente, vocabulaire) if calculer_distance_edition(mot, corr)[1] <= distance_max}\n",
    "                modifications_actuelles.update(nouvelles_corrections)\n",
    "            modifications_precedentes = modifications_actuelles\n",
    "        corrections_possibles = modifications_precedentes\n",
    "\n",
    "    return corrections_possibles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9557800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compter_mots(phrases_tokenisees: list[list[str]]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Compte le nombre d'apparitions de chaque mot dans les phrases tokenisées.\n",
    "\n",
    "    Paramètres :\n",
    "        sentences_tokenises (list[list[str]]) : Liste de listes de chaînes de caractères.\n",
    "\n",
    "    Retourne :\n",
    "        word_counts (dict[str, int]) : Dictionnaire qui fait correspondre chaque mot (str) à sa fréquence (int).\n",
    "    \"\"\"\n",
    "\n",
    "    word_counts = {}\n",
    "    for phrase in phrases_tokenisees:\n",
    "        for token in phrase:\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "    return word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9164d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenir_mots_avec_frequence_superieure_ou_egale(phrases_tokenisees: list[list[str]], \n",
    "                                   seuil_frequence: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Trouve les mots qui apparaissent N fois ou plus.\n",
    "\n",
    "    Paramètres :\n",
    "        tokenized_sentences (list[list[str]]) : Liste de listes de phrases.\n",
    "        seuil_frequence (int) : Nombre minimum d'occurrences pour qu'un mot fasse partie du vocabulaire restreint.\n",
    "\n",
    "    Retourne :\n",
    "        closed_vocab (list[str]) : Liste des mots qui apparaissent N fois ou plus.\n",
    "    \"\"\"\n",
    "    closed_vocab = []\n",
    "    word_counts = compter_mots(phrases_tokenisees)\n",
    "    for word, cnt in word_counts.items():\n",
    "        if cnt >= seuil_frequence:\n",
    "            closed_vocab.append(word)\n",
    "\n",
    "    return closed_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99cfe67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remplacer_mots_inconnus_par_unk(phrases_tokenisees: list[list[str]], \n",
    "                             vocabulaire: list[str], unknown_token: str=\"<unk>\"\n",
    "                             ) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with the unknown token.\n",
    "\n",
    "    Parameters:\n",
    "        phrases_tokenisees: List of lists of strings\n",
    "        vocabulaire: List of strings that we will use\n",
    "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
    "    \n",
    "    Returns:\n",
    "        replaced_phrases_tokenisees: List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulaire = set(vocabulaire)\n",
    "    replaced_phrases_tokenisees = []\n",
    "    for phrase in phrases_tokenisees:\n",
    "        replaced_phrase = []\n",
    "        for token in sentence:\n",
    "            if token in vocabulaire:\n",
    "                replaced_phrase.append(token)\n",
    "            else:\n",
    "                replaced_phrase.append(unknown_token)\n",
    "        replaced_phrases_tokenisees.append(replaced_phrase)\n",
    "        \n",
    "    return replaced_phrases_tokenisees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f0dabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def données_avec_unk(données: list[list[str]], seuil_frequence: int):\n",
    "    \"\"\"\n",
    "    Prétraite les données en remplaçant les mots peu fréquents par le jeton inconnu.\n",
    "    \n",
    "    Paramètres :\n",
    "        données (list[list[str]]) : Liste de listes de chaînes de caractères.\n",
    "        freq_threshold (int) : Les mots dont le compte est inférieur à cette valeur sont considérés comme inconnus.\n",
    "\n",
    "    Retourne :\n",
    "        Tuple de\n",
    "        - données avec les mots peu fréquents remplacés par \"<unk>\"\n",
    "        - vocabulaire des mots qui apparaissent au moins n fois dans les données d'entraînement\n",
    "    \"\"\"\n",
    "   \n",
    "    vocabulaire = obtenir_mots_avec_frequence_superieure_ou_egale(phrases_tokenisees, seuil_frequence)\n",
    "    données_remplacées = remplacer_mots_inconnus_par_unk(phrases_tokenisees, vocabulaire)\n",
    "    \n",
    "    return données_remplacées, vocabulaire\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b41cac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compter_n_grammes(données: list[list[str]], n: int=2, \n",
    "                      jeton_début: str='<d>', jeton_fin: str= '<f>') -> dict:\n",
    "    \"\"\"\n",
    "    Compte tous les n-grammes dans les données fournies.\n",
    "    \n",
    "    Paramètres :\n",
    "        données (list[list[str]]) : Liste de listes de mots.\n",
    "        n (int) : nombre de mots dans une séquence (par défaut 2).\n",
    "        jeton_début (str) : une chaîne de caractères indiquant le début de la phrase (par défaut '<d>').\n",
    "        jeton_fin (str) : une chaîne de caractères indiquant la fin de la phrase (par défaut '<f>').\n",
    "    \n",
    "    Retourne :\n",
    "        n_grammes (dict) : Un dictionnaire qui mappe un tuple de n mots à sa fréquence.\n",
    "    \"\"\"\n",
    "    n_grammes = {}\n",
    "\n",
    "    for phrase in données:\n",
    "        phrase = [jeton_début]*n + phrase + [jeton_fin]\n",
    "        phrase = tuple(phrase)\n",
    "        m = len(phrase) if n == 1 else len(phrase) - 1\n",
    "        for i in range(m): \n",
    "            n_gramme = phrase[i:i+n]\n",
    "            if n_gramme in n_grammes.keys():\n",
    "                n_grammes[n_gramme] += 1\n",
    "            else:\n",
    "                n_grammes[n_gramme] = 1\n",
    "    \n",
    "    return n_grammes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7ebb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimer_probabilite(mot: str, n_gramme_precedent: list[str], comptes_n_grammes: dict, \n",
    "                       comptes_n_plus1_grammes: dict, taille_vocabulaire: int, k: float=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Estime les probabilités d'un prochain mot en utilisant les comptes de n-grammes avec lissage k.\n",
    "    \n",
    "    Paramètres :\n",
    "        mot (str) : Prochain mot.\n",
    "        n_gramme_precedent (list[str]) : Une séquence de mots de longueur n.\n",
    "        comptes_n_grammes (dict) : Dictionnaire des comptes des n-grammes.\n",
    "        comptes_n_plus1_grammes (dict) : Dictionnaire des comptes des (n+1)-grammes.\n",
    "        taille_vocabulaire (int) : Nombre de mots dans le vocabulaire.\n",
    "        k (float) : Constante positive, paramètre de lissage (par défaut 1.0).\n",
    "    \n",
    "    Retourne :\n",
    "        probabilité (float) : Une probabilité.\n",
    "    \"\"\"\n",
    "    n_gramme_precedent = tuple(n_gramme_precedent)\n",
    "    comptage_n_gramme_precedent = comptes_n_grammes[n_gramme_precedent] if n_gramme_precedent in comptes_n_grammes else 0\n",
    "    dénominateur = comptage_n_gramme_precedent + k * taille_vocabulaire\n",
    "\n",
    "    n_plus1_gramme = n_gramme_precedent + (mot,)\n",
    "    comptage_n_plus1_gramme = comptes_n_plus1_grammes[n_plus1_gramme] if n_plus1_gramme in comptes_n_plus1_grammes else 0\n",
    "    numérateur = comptage_n_plus1_gramme + k\n",
    "    \n",
    "    probabilité = numérateur / dénominateur\n",
    "        \n",
    "    return probabilité\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f67b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminé !\n"
     ]
    }
   ],
   "source": [
    "def correction_orthographe(texte: str, vocabulaire: set[str], top_n: int=2, n_g: int=2, k: int=1.0,\n",
    "                          n_edits: int=1, max_distance: int=2) -> tuple[dict, str]:\n",
    "    \"\"\"\n",
    "    Effectue une correction orthographique sur le texte donné en utilisant un modèle de langue n-gramme.\n",
    "\n",
    "    Paramètres :\n",
    "        texte (str) : Le texte sur lequel effectuer la correction orthographique.\n",
    "        vocabulaire (set[str]) : Un ensemble de mots représentant le vocabulaire.\n",
    "        top_n (int) : Le nombre de suggestions les plus probables à prendre en compte (par défaut 2).\n",
    "        n_g (int) : L'ordre du modèle de langue n-gramme (par défaut 2).\n",
    "        k (int) : Constante positive, paramètre de lissage (par défaut 1.0).\n",
    "        n_edits (int) : Le nombre maximum de modifications autorisées dans une correction suggérée (par défaut 1).\n",
    "        max_distance (int) : La distance maximale de modification autorisée pour une correction suggérée (par défaut 2).\n",
    "\n",
    "    Retourne :\n",
    "        sorted_dict (dict) : Dictionnaire de suggestions.\n",
    "        texte_corrige (str) : La version corrigée du texte d'entrée.\n",
    "    \"\"\"\n",
    "    n_meilleures = []\n",
    "    suggestions = dict()\n",
    "    phrases, phrases_tokenisees = obtenir_donnees_tokenisees(texte)\n",
    "    n_grammes = compter_n_grammes(phrases_tokenisees, n_g)\n",
    "    n_plus1_grammes = compter_n_grammes(phrases_tokenisees, n_g+1)\n",
    "    texte_corrige = texte\n",
    "    \n",
    "    for phrase in phrases_tokenisees:\n",
    "        index = None\n",
    "        phrase_tmp = ['<d>']*n_g + phrase + ['<f>']\n",
    "        \n",
    "        for token in phrase:\n",
    "            probas = dict()\n",
    "            if not est_mot(token, vocabulaire):\n",
    "                index = phrase_tmp.index(token)\n",
    "                n_gramme_precedent = tuple(phrase_tmp[abs(index-2):index])\n",
    "                comptes_n_grammes = n_grammes[n_gramme_precedent]\n",
    "                corrections = obtenir_corrections(token, vocabulaire, n_edits, max_distance)\n",
    "                corrections = [c for c in corrections if est_mot(c, vocabulaire)]\n",
    "                for corr in corrections:\n",
    "                    proba = estimer_probabilite(corr, n_gramme_precedent, n_grammes,\n",
    "                                               n_plus1_grammes, len(vocabulaire), k)\n",
    "                    probas[corr] = proba\n",
    "                suggestions[token] = probas\n",
    "        \n",
    "        suggestions_triees = {k: dict(sorted(v.items(), key=lambda item: item[1], reverse=True)) for k, v in suggestions.items()}\n",
    "        sorted_dict = {}\n",
    "        for cle, dict_interne in suggestions_triees.items():\n",
    "            dict_interne_trie = dict(sorted(dict_interne.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
    "            sorted_dict[cle] = dict_interne_trie\n",
    "\n",
    "        for cle in sorted_dict.keys():\n",
    "            if cle in texte_corrige:\n",
    "                premier_mot_interne = next(iter(sorted_dict[cle]))\n",
    "                texte_corrige = texte_corrige.replace(cle, premier_mot_interne)\n",
    "\n",
    "    return sorted_dict, texte_corrige\n",
    "\n",
    "print(\"Terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41977921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type de données : <class 'str'>\n",
      "Nombre de lettres : 6,488,665\n",
      "Premières 100 lettres des données\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of The Adventures of Sherlock Holmes\\nby Sir Arthur Conan Doyle\\n(#15 in o'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Dernières 100 lettres des données\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' i, c, ord(c), big[max(0, i-10):min(N, i+10)]\\n        s.add(c)\\n  print s\\n  print [ord(c) for c in s]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Originale texte:\n",
      "i am enjaying time.\n",
      "I actually likod Derek Morris as a Ranger.\n",
      "\n",
      "Corrige texte:\n",
      "i am enjoying time.\n",
      "I actually liked Derek Morris as a Ranger.\n",
      "\n",
      "The misspelled words and thier corrections:\n",
      "--------------------------------------------------\n",
      "  enjaying:\n",
      "enjoying:\t2.531837861103375e-05\n",
      "--------------------------------------------------\n",
      "  likod:\n",
      "liked:\t2.531837861103375e-05\n",
      "--------------------------------------------------\n",
      "  derek:\n",
      "dere:\t2.531837861103375e-05\n"
     ]
    }
   ],
   "source": [
    "donnees = charger_donnees('corpus.txt', affichage=True)\n",
    "phrases, donnees_tokenisees = obtenir_donnees_tokenisees(donnees)\n",
    "donnees_entrainement, donnees_test = diviser_donnees(donnees_tokenisees, 0.8)\n",
    "vocabulaire = obtenir_vocabulaire(donnees_tokenisees)\n",
    "\n",
    "n_gr = 2\n",
    "n_grams=compter_n_grammes(donnees_tokenisees, n_gr)\n",
    "n1_grams=compter_n_grammes(donnees_tokenisees, n_gr+1)\n",
    "\n",
    "text = \"i am enjaying time.\\nI actually likod Derek Morris as a Ranger.\"\n",
    "\n",
    "sorted_dict, texte_corrige = correction_orthographe(text, vocabulaire)\n",
    "\n",
    "print(f\"Originale texte:\\n{text}\\n\")\n",
    "print(f\"Corrige texte:\\n{texte_corrige}\\n\")\n",
    "print(f\"The misspelled words and thier corrections:\")\n",
    "for mot in sorted_dict.keys():\n",
    "    print('-'*50)\n",
    "    print(f\"  {mot}:\")\n",
    "    for c, p in sorted_dict[mot].items():\n",
    "        print(f\"{c}:\\t{p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b8035d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyenchant\n",
      "  Downloading pyenchant-3.2.2-py3-none-win_amd64.whl (11.9 MB)\n",
      "     ---------------------------------------- 11.9/11.9 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pyenchant\n",
      "Successfully installed pyenchant-3.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyenchant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "020bf293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import enchant\n",
    "import itertools\n",
    "import math\n",
    "import nltk.data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "\n",
    "class Sentence_Corrector:\n",
    "    def __init__(self, training_file):\n",
    "        self.laplaceUnigramCounts = collections.defaultdict(lambda: 0)\n",
    "        self.laplaceBigramCounts = collections.defaultdict(lambda: 0)\n",
    "        self.total = 0\n",
    "        self.sentences = []\n",
    "        self.importantKeywords = set()\n",
    "        self.d = enchant.Dict(\"en_US\")\n",
    "        self.tokenize_file(training_file)\n",
    "        self.train()\n",
    "\n",
    "    def tokenize_file(self, file):\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        with open(file) as f:\n",
    "            content = f.read()\n",
    "        for sentence in tokenizer.tokenize(content):\n",
    "            sentence_clean = [i.lower() for i in re.split('[^a-zA-Z]+', sentence) if i]\n",
    "            self.sentences.append(sentence_clean)\n",
    "\n",
    "    def train(self):\n",
    "        for sentence in self.sentences:\n",
    "            sentence.insert(0, '<s>')\n",
    "            sentence.append('</s>')\n",
    "            for i in range(len(sentence) - 1):\n",
    "                token1 = sentence[i]\n",
    "                token2 = sentence[i + 1]\n",
    "                self.laplaceUnigramCounts[token1] += 1\n",
    "                self.laplaceBigramCounts[(token1, token2)] += 1\n",
    "                self.total += 1\n",
    "            self.total += 1\n",
    "            self.laplaceUnigramCounts[sentence[-1]] += 1\n",
    "\n",
    "    def candidate_word(self, word):\n",
    "        suggests = []\n",
    "        for candidate in self.importantKeywords:\n",
    "            if candidate.startswith(word):\n",
    "                suggests.append(candidate)\n",
    "        suggests.append(word)\n",
    "\n",
    "        if len(suggests) == 1:\n",
    "            suggests = self.d.suggest(word)\n",
    "            suggests = [suggest.lower() for suggest in suggests][:4]\n",
    "            suggests.append(word)\n",
    "            suggests = list(set(suggests))\n",
    "\n",
    "        return suggests, len(suggests)\n",
    "\n",
    "    def candidate_sentence(self, sentence):\n",
    "        candidate_sentences = []\n",
    "        words_count = {}\n",
    "        for word in sentence:\n",
    "            candidate_sentences.append(self.candidate_word(word)[0])\n",
    "            words_count[word] = self.candidate_word(word)[1]\n",
    "\n",
    "        candidate_sentences = list(itertools.product(*candidate_sentences))\n",
    "        return candidate_sentences, words_count\n",
    "\n",
    "    def correction_score(self, words_count, old_sentence, new_sentence):\n",
    "        score = 1\n",
    "        for i in range(len(new_sentence)):\n",
    "            if new_sentence[i] in words_count:\n",
    "                score *= 0.95\n",
    "            else:\n",
    "                score *= (0.05 / (words_count[old_sentence[i]] - 1))\n",
    "        return math.log(score)\n",
    "\n",
    "    def noisy_correction_score(self, words_count, old_sentence, new_sentence, error_probability):\n",
    "        score = 1\n",
    "        for i in range(len(new_sentence)):\n",
    "            if new_sentence[i] in words_count:\n",
    "                score *= (1 - error_probability)\n",
    "            else:\n",
    "                score *= (error_probability / (words_count[old_sentence[i]] - 1))\n",
    "        return math.log(score)\n",
    "\n",
    "    def score(self, sentence):\n",
    "        score = 0.0\n",
    "        for i in range(len(sentence) - 1):\n",
    "            if self.laplaceBigramCounts[(sentence[i], sentence[i + 1])] > 0:\n",
    "                score += math.log(self.laplaceBigramCounts[(sentence[i], sentence[i + 1])])\n",
    "                score -= math.log(self.laplaceUnigramCounts[sentence[i]])\n",
    "            else:\n",
    "                score += (math.log(self.laplaceUnigramCounts[sentence[i + 1]] + 1) + math.log(0.4))\n",
    "                score -= math.log(self.total + len(self.laplaceUnigramCounts))\n",
    "        return score\n",
    "\n",
    "    def return_best_sentence(self, old_sentence):\n",
    "        bestScore = float('-inf')\n",
    "        bestSentence = []\n",
    "        old_sentence = [word.lower() for word in old_sentence.split()]\n",
    "        sentences, word_count = self.candidate_sentence(old_sentence)\n",
    "        for new_sentence in sentences:\n",
    "            new_sentence = list(new_sentence)\n",
    "            score = self.noisy_correction_score(word_count, new_sentence, old_sentence, 0.1)  # Error probability of 0.1\n",
    "            new_sentence.insert(0, '<s>')\n",
    "            new_sentence.append('</s>')\n",
    "            score += self.score(new_sentence)\n",
    "            if score >= bestScore:\n",
    "                bestScore = score\n",
    "                bestSentence = new_sentence\n",
    "        bestSentence = ' '.join(bestSentence[1:-1])\n",
    "        return bestSentence, bestScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "439f4050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('according to a research at cambridge university', -53.411284186487)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrector = Sentence_Corrector('corpus.txt')\n",
    "#corrector.return_best_sentence('this is wron spallin word')\n",
    "corrector.return_best_sentence('aoccdrning to a resarch at cmabridge university')\n",
    "#corrector.return_best_sentence('it does not mttaer in waht oredr the ltteers')\n",
    "#corrector.return_best_sentence('the olny important tihng is taht')\n",
    "#corrector.return_best_sentence('Can they leav him my messages')\n",
    "#corrector.return_best_sentence('This used to belong to thew queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f1d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
